model:
  backend: remote
  name: qwen/qwen3-4b:free
  base_url: https://openrouter.ai/api/v1
  api_key_env: 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
  max_new_tokens: 512
  temperature: 0.2
  top_p: 0.9
# Other options:
# - google/gemma-2-9b
# - pankajmathur/orca_mini_3b
# - Qwen/Qwen3-4B-Instruct-2507
ingest:
  max_pages_per_pdf: 2000
  min_chunk_chars: 400
  max_chunk_chars: 2200
  heading_regex: '^\s*(\d+(?:\.\d+){1,6})\s+(.+?)\s*$'

retrieval:
  # retrieval sizes
  bm25_topk: 80
  dense_topk: 80
  fused_topk: 60
  rerank_topk: 12

  # hybrid fusion
  rrf_k: 60
  rrf_w_bm25: 1.0
  rrf_w_dense: 1.0
  rrf_cap_bm25: 80
  rrf_cap_dense: 80

  # toggles
  enable_dense: true
  enable_rerank: true

  # models
  embedding_model: sentence-transformers/all-MiniLM-L6-v2
  reranker_model: cross-encoder/ms-marco-MiniLM-L-6-v2

  # reranker runtime (nếu bạn dùng patch Reranker/answerer như mình gửi)
  reranker_batch_size: 64
  reranker_device: cuda
  reranker_max_length: 512

packing:
  max_context_chars: 25000
  neighbor_window: 1
  include_headings: true

gate:
  # Với cross-encoder, score có thể âm/dương => để -5 thì gần như "luôn pass"
  # Nếu bạn muốn gate chặt hơn, thử 0.0 hoặc 0.15 (tuỳ distribution score của bạn)
  min_rerank_score: -1.25
  enable_query_rewrite: false
  max_rounds: 1

paths:
  raw_specs_dir: data/raw_specs
  chunks_path: data/intermediate/chunks.jsonl
  docstore_path: data/indexes/docstore.sqlite
  bm25_dir: data/indexes/bm25
  faiss_dir: data/indexes/faiss

server:
  host: 0.0.0.0
  port: 8000
